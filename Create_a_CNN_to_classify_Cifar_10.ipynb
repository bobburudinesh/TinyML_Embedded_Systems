{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9p+4IEEaRQMEvBWAqsCSV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobburudinesh/TinyML_Embedded_Systems/blob/main/Create_a_CNN_to_classify_Cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhQg0zktCQQe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVBUpuHXEtw"
      },
      "source": [
        "# Create a CNN to classify Cifar-10\n",
        "\n",
        "Learn about Cifar-10 here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "In class you saw how to build a Convolutional Neural Network that classified Fashion MNIST. Take what you learned to build a CNN that recognizes the 10 classes of CIFAR. It will be a similar network, but there are some key differences you'll need to take into account.\n",
        "\n",
        "First, while MNIST were 28x28 monochome images (1 color channel), CIFAR are 32x32 color images (3 color channels).\n",
        "\n",
        "Second, MNIST images are simple, containing just the object, centered in the image, with no background. CIFAR ones can have the object with a background -- for example airplanes might have a cloudy sky behind them! As such you should expect your accuracy to be a bit lower.\n",
        "\n",
        "We start by setting up the problem for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OQ_tVTaU3oo",
        "outputId": "3723b9d7-8f84-4a4b-d9a3-1976cd7708d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPKVO3DF9bNb"
      },
      "source": [
        "We then definte some of the model for you but leave most of it for you to fill in!\n",
        "\n",
        "*A hint: your model may want to learn some high level features and then classify them.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "law2hNzdVA16",
        "outputId": "56cef5fa-5b0c-47c6-8889-c91ffcbaaa1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "FIRST_LAYER = tf.keras.layers.Conv2D(32,(3,3), activation='relu', input_shape=(32,32,3))\n",
        "HIDDEN_LAYER_TYPE_1 = tf.keras.layers.MaxPooling2D((2,2))\n",
        "HIDDEN_LAYER_TYPE_2 = tf.keras.layers.Conv2D(64,(3,3), activation='relu')\n",
        "HIDDEN_LAYER_TYPE_3 = tf.keras.layers.MaxPooling2D((2,2))\n",
        "HIDDEN_LAYER_TYPE_4 = tf.keras.layers.Conv2D(64,(3,3), activation='relu')\n",
        "HIDDEN_LAYER_TYPE_5 = tf.keras.layers.Dense(64, activation = 'relu')\n",
        "LAST_LAYER = tf.keras.layers.Dense(10)\n",
        "\n",
        "model = models.Sequential([\n",
        "       FIRST_LAYER,\n",
        "       HIDDEN_LAYER_TYPE_1,\n",
        "       HIDDEN_LAYER_TYPE_2,\n",
        "       HIDDEN_LAYER_TYPE_3,\n",
        "       HIDDEN_LAYER_TYPE_4,\n",
        "       layers.Flatten(),\n",
        "       HIDDEN_LAYER_TYPE_5,\n",
        "       LAST_LAYER,\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCSnffa9bNb"
      },
      "source": [
        "You then need to define loss function. And you can then train your model. Once training is done you'll see a plot of training and validation accuracy. You'll know you have a reasonable model with a reasonable loss funciton if your final training accuracy ends up in the 70s (or possibly higher).\n",
        "\n",
        "*A hint: your model may want to learn different categories.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxq2JU7dVIEX"
      },
      "outputs": [],
      "source": [
        "LOSS = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "NUM_EPOCHS = 20 #You can change this value if you like to experiment with it to get better accuracy\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=LOSS,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_images, train_labels, epochs=NUM_EPOCHS,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.xlim([0,NUM_EPOCHS])\n",
        "plt.ylim([0.4,1.0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtrk-cp79bNb"
      },
      "source": [
        "Finally, pick a better optimizer. And re-train your model. You'll know you have a reasonable model with a reasonable loss funciton and optimizer if your final training accuracy ends up in the 80s (or possibly higher).\n",
        "\n",
        "*A hint: your model may want to learn adaptively.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWd9jujr9bNc"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER = #YOUR CODE HERE#\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=OPTIMIZER,\n",
        "              loss=LOSS,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_images, train_labels, epochs=NUM_EPOCHS,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.xlim([0,NUM_EPOCHS])\n",
        "plt.ylim([0.4,1.0])\n",
        "plt.show()"
      ]
    }
  ]
}